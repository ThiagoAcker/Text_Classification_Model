{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "619d03a8-af68-4e78-8096-eda5e8033e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                News Category\n",
      "0  The IMF now expects China's economy to grow by...  Economy\n",
      "1  Manufacturing activity in the Eurozone has dec...  Economy\n",
      "2  Continued disruptions in the global supply cha...  Economy\n",
      "3  Concerns about food security remain high due t...  Economy\n",
      "4  While some central banks, like the US Federal ...  Economy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "df = pd.read_csv(r'C:\\Users\\acker\\Datasets\\News_Categoires.csv', encoding='latin-1')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20be7e84-829d-46fa-9427-b088450cfdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Final: 70.59%\n",
      "['Health' 'Food' 'Entertainment' 'Economy' 'Economy' 'Entertainment'\n",
      " 'Economy' 'Entertainment' 'Economy' 'Entertainment' 'Entertainment'\n",
      " 'Health' 'International relations' 'Food' 'Food' 'Entertainment'\n",
      " 'Politics' 'International relations' 'Food' 'Entertainment'\n",
      " 'Artificial Intelligence ' 'International relations' 'Food'\n",
      " 'International relations' 'Economy' 'Entertainment' 'Politics'\n",
      " 'Artificial Intelligence ' 'Artificial Intelligence ' 'Economy' 'Economy'\n",
      " 'International relations' 'International relations' 'Sports' 'Sports'\n",
      " 'Sports' 'International relations' 'Health' 'Sports' 'Health' 'Sports'\n",
      " 'Sports' 'Entertainment' 'International relations' 'Entertainment'\n",
      " 'Entertainment' 'Economy' 'Artificial Intelligence ' 'Health'\n",
      " 'Entertainment' 'Artificial Intelligence ']\n"
     ]
    }
   ],
   "source": [
    "X = df['News']\n",
    "y = df['Category']\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english') # Remove palavras irrelevantes\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "acuracia = accuracy_score(y_test, y_pred)\n",
    "print(f\"Acurácia Final: {acuracia:.2%}\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee315ff2-606a-4ef1-8e3a-668c3017d869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucesso! O arquivo 'Resultado_Classificacao_Final.xlsx' foi gerado.\n",
      "                                    Noticia_Original Categoria_Predita  \\\n",
      "0  ï»¿Golf legend Tiger Woods returned to competi...            Sports   \n",
      "1  Novak Djokovic claimed his seventh Wimbledon t...            Sports   \n",
      "2  The highly anticipated rubber match between UF...            Sports   \n",
      "3  The US economy shrank at an annualized rate of...           Economy   \n",
      "4  Central banks around the world, including the ...           Economy   \n",
      "\n",
      "  Confianca  \n",
      "0    32.99%  \n",
      "1    18.15%  \n",
      "2    32.33%  \n",
      "3    37.14%  \n",
      "4    42.34%  \n"
     ]
    }
   ],
   "source": [
    "df_samples = df_samples.rename(columns={'raw_data': 'news'})\n",
    "\n",
    "# 2. Definir a variável de entrada\n",
    "new_samples = df_samples[\"news\"]\n",
    "\n",
    "# 3. Vetorização (Usando o vectorizer já treinado anteriormente)\n",
    "# IMPORTANTE: Se der erro de \"NaN\", removemos linhas vazias antes\n",
    "new_samples = new_samples.dropna().astype(str)\n",
    "new_samples_vetorizados = vectorizer.transform(new_samples)\n",
    "\n",
    "# 4. Predição e Confiança\n",
    "predicoes = model.predict(new_samples_vetorizados)\n",
    "confianca = model.predict_proba(new_samples_vetorizados).max(axis=1)\n",
    "\n",
    "# 5. Criar o DataFrame Final\n",
    "df_final = pd.DataFrame({\n",
    "    'Noticia_Original': new_samples,\n",
    "    'Categoria_Predita': predicoes,\n",
    "    'Confianca': confianca\n",
    "})\n",
    "\n",
    "# Formatar confiança para porcentagem\n",
    "df_final['Confianca'] = df_final['Confianca'].map('{:.2%}'.format)\n",
    "\n",
    "# 6. Exportar para Excel\n",
    "df_final.to_excel(\"Resultado_Classificacao_Final.xlsx\", index=False)\n",
    "\n",
    "print(\"Sucesso! O arquivo 'Resultado_Classificacao_Final.xlsx' foi gerado.\")\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d4e8342-700a-4faa-9158-00c001efd1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O arquivo foi salvo nesta pasta:\n",
      "C:\\Users\\acker\\Python Projects\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. Descobrir onde o arquivo foi salvo\n",
    "print(\"O arquivo foi salvo nesta pasta:\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1fe93b40-a937-4db6-9a6b-c538e3da46a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'news'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'news'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 4. Criar o DataFrame com uma única coluna \"Crua\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m df_samples \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(conteudo_bruto, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 18\u001b[0m new_samples \u001b[38;5;241m=\u001b[39m \u001b[43mdf_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnews\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 2. IMPORTANTE: Transforme os novos textos usando o MESMO vectorizer\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Usamos apenas .transform(), NUNCA .fit_transform() aqui\u001b[39;00m\n\u001b[0;32m     22\u001b[0m new_samples_vetorizados \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(new_samples)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'news'"
     ]
    }
   ],
   "source": [
    "# 1. Caminho do arquivo\n",
    "arquivo = r'C:\\Users\\acker\\Datasets\\News_Categoires_Samples.csv'\n",
    "\n",
    "# 2. Lista para guardar o conteúdo bruto\n",
    "conteudo_bruto = []\n",
    "\n",
    "# 3. Ler o arquivo linha a linha como texto simples (ignorando regras de CSV)\n",
    "with open(arquivo, 'r', encoding='latin-1') as f:\n",
    "    for linha in f:\n",
    "        # Removemos espaços em branco nas pontas e quebras de linha\n",
    "        linha_limpa = linha.strip()\n",
    "        if linha_limpa: # Se a linha não estiver vazia\n",
    "            conteudo_bruto.append(linha_limpa)\n",
    "\n",
    "# 4. Criar o DataFrame com uma única coluna \"Crua\"\n",
    "df_samples = pd.DataFrame(conteudo_bruto, columns=['raw_data'])\n",
    "\n",
    "new_samples = df_samples[\"news\"]\n",
    "\n",
    "# 2. IMPORTANTE: Transforme os novos textos usando o MESMO vectorizer\n",
    "# Usamos apenas .transform(), NUNCA .fit_transform() aqui\n",
    "new_samples_vetorizados = vectorizer.transform(new_samples)\n",
    "\n",
    "# 3. Use o modelo para prever as categorias\n",
    "predicoes = model.predict(new_samples_vetorizados)\n",
    "\n",
    "# 4. Visualize o resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac516c-3edc-4bbf-81be-7a262f51f95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46133c-1c74-4b8a-975e-332cd417f6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
